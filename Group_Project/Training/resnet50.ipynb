{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepair data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths to data\n",
    "data_dir = 'Data/fashionIQ_dataset/fashionIQ_dataset'\n",
    "image_dir = os.path.join(data_dir, 'images')\n",
    "json_dir = os.path.join(data_dir, 'image_splits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Init functions for prepair and pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read data files\n",
    "\n",
    "# read data in json file\n",
    "def read_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "#  load image (path of image) from file json (follow cattegory)\n",
    "def load_image_list(category, split):\n",
    "    json_path = os.path.join(json_dir, f'split.{category}.{split}.json')\n",
    "    image_list = read_json(json_path)\n",
    "    return [os.path.join(image_dir, image_name + '.jpg') for image_name in image_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to preprocess data\n",
    "\n",
    "# define transform for preprocess step\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# define class FashionIQDataset \n",
    "class FashionIQDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_dress = load_image_list('dress', 'train')\n",
    "val_images_dress = load_image_list('dress', 'val')\n",
    "test_images_dress = load_image_list('dress', 'test')\n",
    "\n",
    "# category - shirt\n",
    "train_images_shirt = load_image_list('shirt', 'train')\n",
    "val_images_shirt = load_image_list('shirt', 'val')\n",
    "test_images_shirt = load_image_list('shirt', 'test')\n",
    "\n",
    "# category - top&tee\n",
    "train_images_toptee = load_image_list('toptee', 'train')\n",
    "val_images_toptee = load_image_list('toptee', 'val')\n",
    "test_images_toptee = load_image_list('toptee', 'test')\n",
    "\n",
    "# save images\n",
    "dress = train_images_dress + val_images_dress + test_images_dress\n",
    "shirt = train_images_shirt + val_images_shirt + test_images_shirt\n",
    "toptee = train_images_toptee + val_images_toptee + test_images_toptee\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "\n",
    "def saveImage(category, new_path):\n",
    "    index = 1\n",
    "    for path in category:\n",
    "        image = cv.imread(path)\n",
    "        cv.imwrite(new_path + \"_\" + str(index) + \".jpg\", image)\n",
    "        index +=1\n",
    "        \n",
    "saveImage(dress, \"Data/fashionIQ/dress/dress\")\n",
    "        \n",
    "# saveImage(shirt, \"Data/fashionIQ/shirt/shirt\")\n",
    "# saveImage(shirt, \"Data/fashionIQ/top-tee/topntee\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and split data in corresponding set \n",
    "\n",
    "# load images follow their category in three set: train, val and test\n",
    "# category - dress\n",
    "train_images_dress = load_image_list('dress', 'train')\n",
    "val_images_dress = load_image_list('dress', 'val')\n",
    "test_images_dress = load_image_list('dress', 'test')\n",
    "\n",
    "# category - shirt\n",
    "train_images_shirt = load_image_list('shirt', 'train')\n",
    "val_images_shirt = load_image_list('shirt', 'val')\n",
    "test_images_shirt = load_image_list('shirt', 'test')\n",
    "\n",
    "# category - top&tee\n",
    "train_images_toptee = load_image_list('toptee', 'train')\n",
    "val_images_toptee = load_image_list('toptee', 'val')\n",
    "test_images_toptee = load_image_list('toptee', 'test')\n",
    "\n",
    "# combine all image (in each set) to make a dataset\n",
    "train_images = train_images_dress + train_images_shirt + train_images_toptee\n",
    "val_images = val_images_dress + val_images_shirt + val_images_toptee\n",
    "test_images = test_images_dress + test_images_shirt + test_images_toptee\n",
    "\n",
    "# label for images (0: dress, 1: shirt, 2: toptee)\n",
    "train_labels = [0] * len(train_images_dress) + [1] * len(train_images_shirt) + [2] * len(train_images_toptee)\n",
    "val_labels = [0] * len(val_images_dress) + [1] * len(val_images_shirt) + [2] * len(val_images_toptee)\n",
    "test_labels = [0] * len(test_images_dress) + [1] * len(test_images_shirt) + [2] * len(test_images_toptee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process data\n",
    "train_dataset = FashionIQDataset(train_images, train_labels, transform=transform)\n",
    "val_dataset = FashionIQDataset(val_images, val_labels, transform=transform)\n",
    "test_dataset = FashionIQDataset(test_images, test_labels, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ResNet50 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define neccessary infor and ResNet50 model\n",
    "model_dir = 'models'\n",
    "num_classes = 3\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# init resnet50 model\n",
    "model = models.resnet50(pretrained = True)\n",
    "# modify the last layer to appropriate the number of class in the dataset\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "# put model to device (GPU if has)\n",
    "model = model.to(device)\n",
    "\n",
    "# init criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'models'\n",
    "num_classes = 3\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# init resnet50 model\n",
    "model = models.resnet50(pretrained = True)\n",
    "# modify the last layer to appropriate the number of class in the dataset\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "# put model to device (GPU if has)\n",
    "model = model.to(device)\n",
    "\n",
    "# init criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# init train & evaluate function \n",
    "total_loss, total_val_acc = []\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=10):\n",
    "    best_val_accuracy = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"training on epoch {epoch+1} ...\")\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Xóa gradients của bộ tối ưu hóa\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Tiến hành dự đoán\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Lan truyền ngược và tối ưu hóa\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Đánh giá trên tập validation sau mỗi epoch\n",
    "        val_accuracy = evaluate(model, val_loader, device)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
    "        \n",
    "        total_loss.append(running_loss)\n",
    "        total_val_acc(val_accuracy)\n",
    "        \n",
    "        # Lưu lại mô hình tốt nhất\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            model_save_path = os.path.join(model_dir, 'best_model.pth')\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(\"Saved best model to\", model_save_path)\n",
    "\n",
    "# Hàm đánh giá mô hình\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "train(model, train_loader, val_loader, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Use pre-train weight for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if device is GPU \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_dir = 'models/best_model.pth'\n",
    "num_classes = 3\n",
    "\n",
    "# init resnet50 model as the way training\n",
    "model = models.resnet50(pretrained=False)  # pretrained=False vì bạn sẽ tải trọng số của mình\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# load \n",
    "# model.load_state_dict(torch.load(model_dir))\n",
    "model.load_state_dict(torch.load(model_dir))\n",
    "\n",
    "model.to(device)  \n",
    "# Chuyển mô hình về chế độ đánh giá (evaluation mode)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Code\\Vision-Image-Retrieval\\Group_Project\\.venv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\Code\\Vision-Image-Retrieval\\Group_Project\\.venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_5848\\2018282355.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_dir, map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if device is CPU\n",
    "num_classes = 3\n",
    "model_dir = 'models/best_model.pth'\n",
    "\n",
    "# Tạo mô hình ResNet-50\n",
    "model = models.resnet50(pretrained=False)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# load weight and put on CPU\n",
    "model.load_state_dict(torch.load(model_dir, map_location=torch.device('cpu')))\n",
    "\n",
    "# Chuyển mô hình về chế độ đánh giá (evaluation mode)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_batchwise(loader, model, device, save_path_features, save_path_labels):\n",
    "    model.eval()\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    os.makedirs(os.path.dirname(save_path_features), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(save_path_labels), exist_ok=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, labels) in enumerate(loader):\n",
    "            images = images.to(device)\n",
    "            output = model(images)\n",
    "            output = output.view(output.size(0), -1)\n",
    "            features_list.append(output.cpu().numpy())\n",
    "            labels_list.append(labels.numpy())\n",
    "            \n",
    "            # save feature in batch to save resource\n",
    "            if batch_idx % 10 == 0:  # save after 10 batch\n",
    "                batch_features = np.concatenate(features_list, axis=0)\n",
    "                batch_labels = np.concatenate(labels_list, axis=0)\n",
    "                np.save(f'{save_path_features}_batch_{batch_idx}.npy', batch_features, allow_pickle=False)\n",
    "                np.save(f'{save_path_labels}_batch_{batch_idx}.npy', batch_labels, allow_pickle=False)\n",
    "                features_list = []\n",
    "                labels_list = []\n",
    "            \n",
    "    if features_list:\n",
    "        batch_features = np.concatenate(features_list, axis=0)\n",
    "        batch_labels = np.concatenate(labels_list, axis=0)\n",
    "        np.save(f'{save_path_features}_final.npy', batch_features, allow_pickle=False)\n",
    "        np.save(f'{save_path_labels}_final.npy', batch_labels, allow_pickle=False)\n",
    "        \n",
    "        \n",
    "def merge_npy_files(folder_path, file_pattern, output_file):\n",
    "    sorted_files = sorted(\n",
    "    [f for f in os.listdir(folder_path) if f.startswith(file_pattern + '_batch') and f.endswith('.npy')],\n",
    "    key=lambda x: int(x.split('_')[-1].split('.')[0])\n",
    ")\n",
    "    sorted_files.append(file_pattern + '_final.npy')\n",
    "    arrays = [np.load(os.path.join(folder_path, f)) for f in sorted_files]\n",
    "    \n",
    "    all_array = np.concatenate(arrays, axis=0)\n",
    "    np.save(output_file, all_array, allow_pickle=False)\n",
    "    print(f\"Saved combined features to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Extract feature for all images in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path_features = 'features/features/features'\n",
    "save_path_labels = 'features/labels/labels'\n",
    "\n",
    "all_dataset = FashionIQDataset(train_images + val_images + test_images, train_labels + val_labels + test_labels, transform=transform)\n",
    "all_loader = DataLoader(all_dataset, batch_size=32, shuffle=False)\n",
    "extract_features_batchwise(all_loader, model, device, save_path_features, save_path_labels)\n",
    "\n",
    "merge_npy_files('features/labels', 'labels', 'features/all_labels.npy')\n",
    "merge_npy_files('features/features', 'features', 'features/all_features.npy')\n",
    "\n",
    "# load các file đặc trưng lên\n",
    "all_features = np.load('features/all_features.npy')\n",
    "all_labels = np.load('features/all_labels.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Extract feature for test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_query_accuracy(query_loader, feature_extractor, all_features, all_labels, transform, device, k=5, metric='cosine'):\n",
    "    feature_extractor.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in query_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.numpy()\n",
    "            output_features = feature_extractor(images)\n",
    "            output_features = output_features.view(output_features.size(0), -1).cpu().numpy()\n",
    "\n",
    "            # Tìm các ảnh gần nhất\n",
    "            if metric == 'cosine':\n",
    "                distances = cosine_similarity(output_features, all_features)\n",
    "                distances = 1 - distances  # Chuyển đổi từ similarity sang distance\n",
    "            elif metric == 'euclidean':\n",
    "                distances = euclidean_distances(output_features, all_features)\n",
    "            else:\n",
    "                raise ValueError(\"Metric must be either 'cosine' or 'euclidean'\")\n",
    "\n",
    "            # Dự đoán lớp cho tất cả ảnh trong batch\n",
    "            knn = NearestNeighbors(n_neighbors=k, metric=metric)\n",
    "            knn.fit(all_features)\n",
    "            _, indices = knn.kneighbors(output_features)\n",
    "\n",
    "            for idx_list, true_label in zip(indices, labels):\n",
    "                predicted_labels = [all_labels[idx] for idx in idx_list]\n",
    "                most_common_label = np.bincount(predicted_labels).argmax()\n",
    "                predictions.append(most_common_label)\n",
    "                true_labels.append(true_label)\n",
    "\n",
    "    return accuracy_score(true_labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo DataLoader cho tập kiểm tra\n",
    "test_dataset = FashionIQDataset(test_images, test_labels, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Tính độ chính xác cho mô hình truy vấn\n",
    "print(\"accuracy\")\n",
    "query_accuracy = calculate_query_accuracy(test_loader, model, all_features, all_labels, transform, device, k=5, metric='cosine')\n",
    "print(f\"Query Accuracy: {query_accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
